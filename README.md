# ğŸ•·ï¸ MCP Web Scrape

> Clean, cached web content for agentsâ€”Markdown + citations, robots-aware, ETag/304 caching.

[![npm version](https://badge.fury.io/js/mcp-web-scrape.svg)](https://badge.fury.io/js/mcp-web-scrape)
[![CI](https://github.com/mukul975/mcp-web-scrape/workflows/CI/badge.svg)](https://github.com/mukul975/mcp-web-scrape/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub stars](https://img.shields.io/github/stars/mukul975/mcp-web-scrape.svg?style=social&label=Star)](https://github.com/mukul975/mcp-web-scrape)

## ğŸ¬ Demo

<!-- Replace with actual GIF -->
<!-- Demo GIF will be added soon -->
> ğŸ¬ **Demo coming soon!** A comprehensive video demonstration showcasing the MCP Web Scrape server's capabilities will be available here.

*Watch how MCP Web Scrape transforms messy HTML into clean Markdown with citations in seconds*

## âš¡ Quick Start

```bash
# Install globally
npm install -g mcp-web-scrape

# Try it instantly
npx mcp-web-scrape@latest

# Or start HTTP server
node dist/http.js
```

### ChatGPT Desktop Setup

Add to your `~/Library/Application Support/ChatGPT/config.json`:

```json
{
  "mcpServers": {
    "web-scrape": {
      "command": "npx",
      "args": ["mcp-web-scrape@latest"]
    }
  }
}
```

### Claude Desktop Setup

Add to your `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "web-scrape": {
      "command": "npx",
      "args": ["mcp-web-scrape@latest"]
    }
  }
}
```

## ğŸ› ï¸ Available Tools

| Tool | Description |
|------|-------------|
| `extract_content` | Convert HTML to clean Markdown with citations |
| `summarize_content` | AI-powered content summarization |
| `get_page_metadata` | Extract title, description, author, keywords |
| `extract_links` | Get all links with filtering options |
| `extract_images` | Extract images with alt text and dimensions |
| `search_content` | Search within page content |
| `check_url_status` | Verify URL accessibility |
| `validate_robots` | Check robots.txt compliance |
| `extract_structured_data` | Parse JSON-LD, microdata, RDFa |
| `compare_content` | Compare two pages for changes |
| `batch_extract` | Process multiple URLs efficiently |
| `get_cache_stats` | View cache performance metrics |
| `clear_cache` | Manage cached content |

## ğŸ¤” Why Not Just Use Built-in Browsing?

**Deterministic Results** â†’ Same URL always returns identical content  
**Smart Citations** â†’ Every fact links back to its source  
**Robots Compliant** â†’ Respects robots.txt and rate limits  
**Lightning Fast** â†’ ETag/304 caching + persistent storage  
**Agent-Optimized** â†’ Clean Markdown instead of messy HTML  

## ğŸ”’ Safety First

- âœ… **Respects robots.txt** by default
- âœ… **Rate limiting** prevents server overload
- âœ… **No paywall bypass** - ethical scraping only
- âœ… **User-Agent identification** for transparency

## ğŸš€ Roadmap

- [x] **Core Web Scraping** - Extract clean content from web pages
- [x] **Markdown Conversion** - Transform HTML to readable Markdown
- [x] **Citation System** - Link extracted content to sources
- [x] **Caching System** - Persistent storage with ETag support
- [x] **Rate Limiting** - Respectful scraping with configurable delays
- [x] **Robots.txt Compliance** - Ethical scraping practices
- [ ] **Playwright Integration** - JavaScript rendering for SPAs
- [ ] **PDF Snapshots** - Archive pages as searchable PDFs
- [ ] **Cache Viewer UI** - Web interface for cache management
- [ ] **Custom Extractors** - Plugin system for specialized content
- [ ] **Batch Processing** - Queue system for large-scale extraction

## ğŸ“¦ Installation

```bash
npm install -g mcp-web-scrape

# Or use directly
npx mcp-web-scrape@latest
```

## ğŸ”§ Configuration

```bash
# Environment variables
export MCP_WEB_SCRAPE_CACHE_DIR="./cache"
export MCP_WEB_SCRAPE_USER_AGENT="MyBot/1.0"
export MCP_WEB_SCRAPE_RATE_LIMIT="1000"
```

## ğŸŒ Transports

**STDIO** (default)
```bash
mcp-web-scrape
```

**HTTP/SSE**
```bash
node bin/http.js --port 3000
```

## ğŸ“š Resources

Access cached content as MCP resources:

```
cache://example.com/path â†’ Cached page content
cache://stats â†’ Cache statistics
cache://robots/example.com â†’ Robots.txt status
```

## ğŸ¤ Contributing

We love contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Good First Issues:**
- Add new content extractors
- Improve error handling
- Write more tests
- Enhance documentation

## ğŸ“„ License

MIT Â© [Mahipal](https://github.com/mukul975)

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=mukul975/mcp-web-scrape&type=Date)](https://star-history.com/#mukul975/mcp-web-scrape&Date)

---

*Built with â¤ï¸ for the [Model Context Protocol](https://modelcontextprotocol.io) ecosystem*